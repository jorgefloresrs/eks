
version: 0.2

environment_variables:
  plaintext:
    AMX_PPL_REGION: "us-east-1"
    AMX_PPL_ENV: "DES"
    AMX_PPL_CLUSTER_EKS: "prueba-amx"
    AMX_PPL_NAMESPACE: "ppl-des"
    AMX_PPL_VPC_ID: "vpc-0c9d013543a933d4a"
    AMX_PPL_ECR_REPO: "602401143452.dkr.ecr.us-east-1.amazonaws.com"
    LOGGROUPNAME: "/paperless/AMX-PPL-CC-DES-CW-LOGGROUP-PPL-AMX"
    LOGGROUPPREFIX: "/paperless/AMX-PPL-CC-DES-CW-LOGGROUP-PPL-AMX-1"
    DEPLOYMENTNAME: "prueba-amx-dpl"
    FLUENTBITURL: "https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml"
phases:
  install:
    commands:
      - |
        curl --silent \
             --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
      - mv -vf /tmp/eksctl /usr/local/bin
      - chmod +x /usr/local/bin/eksctl
      - curl -LO https://dl.k8s.io/release/v1.23.16/bin/linux/amd64/kubectl
      - mv -vf kubectl /usr/local/bin
      - chmod +x /usr/local/bin/kubectl
      - kubectl version --client --output=yaml
      - curl --silent --location https://get.helm.sh/helm-v3.10.2-linux-amd64.tar.gz  | tar xz -C /tmp
      - mv /tmp/linux-amd64/helm /usr/local/bin && chmod +x /usr/local/bin/helm
      - helm version --short
      - curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
      - unzip -u awscliv2.zip
      - ./aws/install --bin-dir /root/.pyenv/shims/ --install-dir /usr/local/aws-cli --update
      - aws --version
  pre_build:
    commands:
      - export ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output=text)
      - EKS_KUBECTL_ROLE_ARN="arn:aws:iam::${ACCOUNT_ID}:role/AMX-PPL-CB-EKS-KUBECTL-766819176966-us-east-1"
      - export AWS_ACCESS_KEY_ID=ASIA3FCP4CIDFCEHQ3XE
      - export AWS_SECRET_ACCESS_KEY=aRlU1AH1eYZmOygz4onZbhBW+ZOlH55MybckR+MR
      - export AWS_SESSION_TOKEN=IQoJb3JpZ2luX2VjEMr//////////wEaCXVzLWVhc3QtMSJIMEYCIQDbqWOI7rkkZZrsLM2Xg3nVpUNG64B7E8mzr+04dAbmowIhAPZcqLqBGFJJsnXIJJE2yyMV74GG8pkdtiSXRF8JYXnzKsADCLP//////////wEQAxoMNzY2ODE5MTc2OTY2IgwTBflZKapmh20u+60qlAOvs7GVSMq9XeeFomtj6G2Vrufa6ALW6fdbHxEFREIt3C4nsRV3N6t1dO0dlnwl/UrOM4F8yK/9wWmr3KOCKW6IiRYiIWKz950yqjg/BEoDtcmdVjWh1DJAkwqth97LIDksCEGizK8J8/XGRKnySSnKrAchdQ0lLy3uPQgB2rolzlHcX5WYZwpOshPQs/9/Wonh1R7dQcgx0EXyOlTErKWYlnH1ARXIZ5DG1ErcLTfpq5jI4orQLBKWKigokMS8BYRGc0N1k7xHlzoFi8nMxsMU/6wfcYbXZJCtrHxhaKUMwZIfq7/yAffxbAuEIuHnume7zQdTmOpOotHRrJROm1ehmpQIfH5h3+IminWge2Djmle8KwCgcK1PGnq4qF/EhETsuIhRVqjciUGjwKvLl79tALBC4b8t1xdeKf+9FobdvuNuk1zsiySpmLmiPEixNY6ETNXoa8E8y8ll6QxhZJg6X/6WqtOtxAQl87HF85ilDxkJFUpofgqwZ5SqDaETu8ANhIxEnXZCUvd0q/qyAixHaIZ25DDPnNihBjrcAUXGbGojvffuF1PRaqcKGuxjQzDHQngx9TcxgkguLo3c2pjRtiBIIzIjNOBSH76r/pH3gk9rT20Zi1UD5TszuUuABZz2kERdkd+dRJPNw8VoH2Pnp0CvRQb9p2MWYWoyh/8WItK42yDtIhYKMHiuBu/5jPecfyFIUFdnTPlg5OzZegqmqEelKrFur8/FNPMhTMzoiw4Kfa2yg9YlSnnzhf4Gx2vLbv8IXLLjArXZndiyTM3jnK5vsDU+KdmOQ9xfR0VLnkYvNEj4UMug5JdiradDJVEwpCyWfrvNnqU=
      - |
        aws eks update-kubeconfig            \
          --name $AMX_PPL_CLUSTER_EKS        \
          --region $AMX_PPL_REGION
      - |
        ROLE="    - groups:\n        - system:masters\n      rolearn: ${EKS_KUBECTL_ROLE_ARN}\n      username: codebuild-kubectl"
        kubectl get -n kube-system configmap/aws-auth -o yaml | awk "/mapRoles: \|/{print;print \"${ROLE}\";next}1" > /tmp/aws-auth-patch.yml
        kubectl patch configmap/aws-auth -n kube-system --patch "$(cat /tmp/aws-auth-patch.yml)"
      - oidc_provider=$(aws eks describe-cluster --name $AMX_PPL_CLUSTER_EKS--region $AMX_PPL_REGION --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")
      - |
        if [ ! ${oidc_provider} ]
        then
          eksctl utils associate-iam-oidc-provider \
            --region $AMX_PPL_REGION               \
            --cluster $AMX_PPL_CLUSTER_EKS         \
            --approve
        fi
  build:
    commands:
      - |
        aws eks update-kubeconfig            \
          --name $AMX_PPL_CLUSTER_EKS        \
          --role-arn ${EKS_KUBECTL_ROLE_ARN} \
          --region $AMX_PPL_REGION
      ### Install AWS Load Controller
      - helm repo add eks https://aws.github.io/eks-charts
      - helm repo update
      - kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"
      - curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.5/docs/install/iam_policy.json
      - AWSLoadBalancerPolicy=$(aws iam get-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy | awk '{print $2}' 2>/dev/null)
      - |
        if [ ! ${AWSLoadBalancerPolicy} ]
        then
          aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json
        fi
      - rm -f iam_policy.json
      - |
        if [ ! $(kubectl get serviceaccounts -n kube-system aws-load-balancer-controller | grep -v "^NAME" | grep -v "^NAME" | wc -l) = 0 ]
        then
          eksctl create iamserviceaccount \
            --cluster $AMX_PPL_CLUSTER_EKS \
            --namespace kube-system \
            --name aws-load-balancer-controller \
            --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy \
            --override-existing-serviceaccounts \
            --region $AMX_PPL_REGION --approve
        fi
      - |
        if [ ! $(kubectl get deployment -n kube-system aws-load-balancer-controller | grep -v "^NAME" | awk '{print $1}' 2>/dev/null) ]
        then
          helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system \
            --set clusterName=$AMX_PPL_CLUSTER_EKS \
            --set region=$AMX_PPL_REGION \
            --set vpcId=$AMX_PPL_CLUSTER_VPC \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set image.repository=$AMX_PPL_ECR_REPO/amazon/aws-load-balancer-controller
        fi
      ### Install Fluent Bit
      - ClusterName=$AMX_PPL_CLUSTER_EKS 
      - RegionName=$AMX_PPL_REGION 
      - FluentBitHttpPort='2020'
      - FluentBitReadFromHead='Off'
      - |
        if [ ${FluentBitReadFromHead} = 'On' ]
        then
          FluentBitReadFromTail='Off'
        else
          FluentBitReadFromTail='On'
        fi
      - |
        if [ -z ${FluentBitHttpPort} ]
        then
          FluentBitHttpServer='Off'
        else
          FluentBitHttpServer='On'
        fi
      - kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/cloudwatch-namespace.yaml
      - |
        if [ ! $(kubectl get configmap/fluent-bit-cluster-info -n amazon-cloudwatch | grep -v "^NAME" | awk '{print $1}' 2>/dev/null) ]
        then
          kubectl create configmap fluent-bit-cluster-info \
            --from-literal=cluster.name=${ClusterName} \
            --from-literal=http.server=${FluentBitHttpServer} \
            --from-literal=http.port=${FluentBitHttpPort} \
            --from-literal=read.head=${FluentBitReadFromHead} \
            --from-literal=read.tail=${FluentBitReadFromTail} \
            --from-literal=logs.region=${RegionName} -n amazon-cloudwatch
        fi
      - |
        if [ ! $(kubectl get serviceaccounts -n amazon-cloudwatch fluent-bit | grep -v "^NAME" | wc -l) = 0 ]
        then 
          curl -o fluent-bit-config.yaml https://github.com/madebybk/eksapp/blob/main/fluent-bit/fluent-bit-config.yaml
          kubectl apply -f fluent-bit-config.yaml
          rm -rf fluent-bit-config.yaml
        fi
      - curl -o permissions.json https://raw.githubusercontent.com/aws-samples/amazon-eks-fluent-logging-examples/mainline/examples/fargate/cloudwatchlogs/permissions.json
      - FargateLoggingPolicy=$(aws iam get-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/eks-fargate-logging-policy | awk '{print $2}' 2>/dev/null)
      - |
        if [ ! ${FargateLoggingPolicy} ]
        then
          aws iam create-policy \
            --policy-name eks-fargate-logging-policy \
            --policy-document file://permissions.json

          aws iam attach-role-policy \
            --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/eks-fargate-logging-policy \
            --role-name AMX-PPL-CC-${AMX_PPL_ENV}-FG-PPL-ROLE
        fi
      - |
        curl ${FLUENTBITURL} | sed 's/{{cluster_name}}/'${ClusterName}'/            
                                    s/{{region_name}}/'${RegionName}'/
                                    s/{{http_server_toggle}}/"'${FluentBitHttpServer}'"/
                                    s/{{http_server_port}}/"'${FluentBitHttpPort}'"/
                                    s/{{read_from_head}}/"'${FluentBitReadFromHead}'"/
                                    s/{{read_from_tail}}/"'${FluentBitReadFromTail}'"/' | kubectl apply -f -
      - |
        if [ ! $(aws eks list-fargate-profiles --cluster-name $AMX_PPL_CLUSTER_EKS --output text | grep fargate-container-insights | awk '{print $2}') ]
        then
          eksctl create fargateprofile             \
            --cluster ${ClusterName}               \
            --name fargate-container-insights      \
            --namespace fargate-container-insights \
            --region $AMX_PPL_REGION
        fi  
      ### Install Otel
      - |
        if [ ! $(aws eks list-addons --cluster-name $AMX_PPL_CLUSTER_EKS --output text) ]
        then
          kubectl apply -f https://amazon-eks.s3.amazonaws.com/docs/addons-otel-permissions.yaml
          aws eks create-addon \
            --addon-name adot \
            --addon-version v0.45.0-eksbuild.1 \
            --cluster-name $AMX_PPL_CLUSTER_EKS
        fi
      - |
        if [ ! $(kubectl get serviceaccounts -n fargate-container-insights adot-collector | grep -v "^NAME" | wc -l) = 0 ]
        then
          eksctl create iamserviceaccount \
            --cluster $AMX_PPL_CLUSTER_EKS \
            --region $AMX_PPL_REGION \
            --namespace fargate-container-insights \
            --name adot-collector \
            --role-name EKS-Fargate-ADOT-ServiceAccount-Role \
            --attach-policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy \
            --approve
        fi
      - |
        if [ ! $(kubectl get serviceaccounts -n $AMX_PPL_NAMESPACE $AMX_PPL_NAMESPACE | grep -v "^NAME" | wc -l) = 0 ]
        then 
          eksctl create iamserviceaccount \
            --name $AMX_PPL_NAMESPACE \
            --namespace $AMX_PPL_NAMESPACE \
            --cluster $AMX_PPL_CLUSTER_EKS \
            --role-name $AMX_PPL_NAMESPACE-ROLE \
            --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AMX-P-PPL-MAR \
            --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AMX-P-PPL-SAC \
            --attach-policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/AMX-P-PPL-SCRT \
            --approve \
            --override-existing-serviceaccounts
        fi
      ### CloudWatch Log configuration
      - sed -i.bk 's/LOGGROUPNAME_PLACEHOLDER/$LOGGROUPNAME/g' manifests/aws-logging-cloudwatch-configmap.yaml
      - sed -i.bk 's/LOGGROUPPREFIX_PLACEHOLDER/$LOGGROUPPREFIX/g' manifests/aws-logging-cloudwatch-configmap.yaml
      ### Installation HPA
      - |
        if [ ! $(kubectl get serviceaccounts -n kube-system metrics-server | grep -v "^NAME" | wc -l) = 0 ]
        then
          kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
        fi
      - sed -i.bk 's/DEPLOYMENTNAME_PLACEHOLDER/$DEPLOYMENTNAME/g' manifests/hpa-cpu.yaml
  post_build:
    commands:
      - kubectl get pods -A -o wide
artifacts:
  files:
    - manifests/**/*
